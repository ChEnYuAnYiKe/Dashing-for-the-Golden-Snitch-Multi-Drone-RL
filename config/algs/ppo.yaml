# Description: Configuration file for the PPO algorithm

# --- RL hyperparameters ---
rl_hyperparams:
  policy: "MlpPolicy" # Policy network architecture
  # env: "" # Environment type, class or name
  learning_rate: 0.0003 # Learning rate for agents
  n_steps: 2048 # Number of steps to run for each agent
  batch_size: 64 # Number of samples per minibatch
  n_epochs: 10 # Number of epochs to train for
  gamma: 0.99 # Discount factor
  gae_lambda: 0.95 # GAE lambda
  clip_range: 0.2 # PPO clip range
  clip_range_vf: None # Value function clip range
  normalize_advantage: True # Normalize advantage function
  ent_coef: 0.0 # Entropy coefficient
  vf_coef: 0.5 # Value function coefficient
  max_grad_norm: 0.5 # Maximum gradient norm
  use_sde: False # Use state dependent exploration
  sde_sample_freq: -1 # Sample a new noise matrix every n steps
  rollout_buffer_class: None # Rollout buffer class
  rollout_buffer_kwargs: None # Rollout buffer kwargs
  target_kl: None # Target KL divergence
  stats_window_size: 100 # Window size for logging stats
  # tensorboard_log: None # Log to tensorboard
  policy_kwargs: None # Additional arguments to pass to the policy
  verbose: 1 # Verbosity level
  # seed: None # Seed for the pseudo-random generators
  # device: "auto" # Device (cpu, cuda, auto)
  _init_setup_model: True # Whether to build the network at the creation of the object

# --- Agent parameters ---
agent:
  net_arch:
    pi: [64, 64] # Policy network's hidden layers
    vf: [64, 64] # Value network's hidden layers
  activation_fn: "Tanh" # Activation function