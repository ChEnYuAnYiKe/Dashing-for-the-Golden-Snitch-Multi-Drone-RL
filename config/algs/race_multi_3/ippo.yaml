# Description: Configuration file for the IPPO algorithm

# --- RL hyperparameters ---
rl_hyperparams:
  policy: "MlpSharePolicy" # Policy network architecture
  # env: "" # Environment type, class or name
  learning_rate: 0.0003 # Learning rate for agents
  n_steps: 2048 # Number of steps to run for each agent
  batch_size: 4096 # Number of samples per minibatch
  n_epochs: 5 # Number of epochs to train for
  gamma: 0.99 # Discount factor
  gae_lambda: 0.95 # GAE lambda
  clip_range: 0.2 # PPO clip range
  clip_range_vf: None # Value function clip range
  normalize_advantage: True # Normalize advantage function
  ent_coef: 0.0 # Entropy coefficient
  vf_coef: 0.5 # Value function coefficient
  max_grad_norm: 0.5 # Maximum gradient norm
  use_sde: False # Use state dependent exploration
  sde_sample_freq: -1 # Sample a new noise matrix every n steps
  rollout_buffer_class: None # Rollout buffer class
  rollout_buffer_kwargs: None # Rollout buffer kwargs
  target_kl: None # Target KL divergence
  stats_window_size: 100 # Window size for logging stats
  # tensorboard_log: None # Log to tensorboard
  policy_kwargs: None # Additional arguments to pass to the policy
  verbose: 1 # Verbosity level
  # seed: None # Seed for the pseudo-random generators
  # device: "auto" # Device (cpu, cuda, auto)
  _init_setup_model: True # Whether to build the network at the creation of the object

# --- MARL hyperparameters ---
marl_hyperparams:
  num_agent: 3 # Number of agents
  use_share_obs: True # Do not share observations
  use_active_masks: True # Do not use active masks
  use_share_obs_env: True # Do not share observations environment type
  use_valuenorm: True # Use value normalization

# --- Agent parameters ---
agent:
  net_arch:
    pi: [128, 128] # Policy network's hidden layers
    vf: [256, 256] # Value network's hidden layers
  activation_fn: "Tanh" # Activation function
  output_activation_fn: "Tanh" # Activation function for the output layer