# All the settings for training the model are defined here

# --- Defaults ---
seed: 0

# --- pyrl options ---
pyrl:
  exp_name: None # Experiment name
  runner: "episode" # Runs 1 env for an episode
  n_envs: 1 # Number of environments to run in parallel
  t_max: 3000000 # Stop running after this many timesteps
  use_cuda: True # Use gpu by default unless it isn't available

# --- Environment options ---
env:
  env_id: "hover-env-v0" # Environment name
  env_kwargs: None # Arguments for the environment

# --- Logging options ---
logging:
  use_tensorboard: True # Log results to tensorboard
  save_model: True # Save the models to disk
  save_ckpt: True # Save the checkpoints to disk
  save_config: True # Save the config file to disk
  save_model_num: 12 # The number of checkpoints to save during training
  load_model_path: None # Load a model from this path
  load_ckpt_path: None # Load a checkpoint from this path
  load_step: 0 # Load model trained on this many timesteps (0 if choose max possible)
  tb_dirname: "Learning_log" # Tensorboard logs directory
  save_model_dirname: "Learning_log" # Save the models to this path
  save_ckpt_dirname: "Learning_log" # Save the checkpoints to this path
  save_config_dirname: "Learning_log" # Save the config file to this path
  save_model_name: "PPO" # The name of the model to save
  log_interval: 1 # Log every n steps
  progress_bar: True # Show progress bar
  reset_num_timesteps: True # Reset the number of timesteps

# --- RL hyperparameters ---
rl_hyperparams:
  policy: "MlpPolicy" # Policy network architecture
  # env: "" # Environment type, class or name
  learning_rate: 0.0003 # Learning rate for agents
  n_steps: 2048 # Number of steps to run for each agent
  batch_size: 64 # Number of samples per minibatch
  n_epochs: 10 # Number of epochs to train for
  gamma: 0.99 # Discount factor
  gae_lambda: 0.95 # GAE lambda
  clip_range: 0.2 # PPO clip range
  clip_range_vf: None # Value function clip range
  normalize_advantage: True # Normalize advantage function
  ent_coef: 0.0 # Entropy coefficient
  vf_coef: 0.5 # Value function coefficient
  max_grad_norm: 0.5 # Maximum gradient norm
  use_sde: False # Use state dependent exploration
  sde_sample_freq: -1 # Sample a new noise matrix every n steps
  rollout_buffer_class: None # Rollout buffer class
  rollout_buffer_kwargs: None # Rollout buffer kwargs
  target_kl: None # Target KL divergence
  stats_window_size: 100 # Window size for logging stats
  # tensorboard_log: None # Log to tensorboard
  policy_kwargs: None # Additional arguments to pass to the policy
  verbose: 0 # Verbosity level
  # seed: None # Seed for the pseudo-random generators
  # device: "auto" # Device (cpu, cuda, auto)
  _init_setup_model: True # Whether to build the network at the creation of the object

# --- MARL hyperparameters ---
# marl_hyperparams:
#   num_agent: 1 # Number of agents
#   use_share_obs: False # Do not share observations
#   use_active_masks: False # Do not use active masks
#   use_share_obs_env: False # Do not share observations environment type
#   use_valuenorm: False # Do not use value normalization

# --- Agent parameters ---
agent:
  net_arch:
    pi: [64, 64] # Policy network's hidden layers
    vf: [64, 64] # Value network's hidden layers
  activation_fn: "Tanh" # Activation function